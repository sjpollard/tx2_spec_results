# Invocation command line:
# /home/br-spollard/SPEChpc/bin/harness/runhpc --rebuild --config=hpe_cpe_gnu_8.cfg --ranks=512 --noreportable --size=test --output_format=all --output_root=/home/br-spollard/benchmarks/tiny/tiny_gnu_512 --iterations=2 tiny
# output_root used was "/home/br-spollard/benchmarks/tiny/tiny_gnu_512"
############################################################################
######################################################################
# Example configuration file for HPE Cray Programming Environment (CPE).
#
# Defines: "model" => "mpi", "omp", default "mpi"
#          "label" => ext base label, default "cce"
#
# Sample MPI-only Command:
# runhpc -c Example_hpe_cpe --reportable -T base --define model=mpi --ranks=48 tiny
#
# Sample OpenMP Command:
# runhpc -c Example_hpe_cpe --reportable -T base --define model=omp --ranks=2 --threads=24 tiny
#
#######################################################################

%ifndef %{label}         # IF label is not set use cce
%   define label cce
%endif

%ifndef %{model}         # IF model is not set use mpi
%   define model mpi
%endif

teeout = yes
makeflags=-j 10

flagsurl000=/home/br-spollard/SPEChpc/config/flags/Example_hpe_cpe_flags.xml

# Tester Information
license_num     = ?
test_sponsor    = University of Bristol
tester          = University of Bristol

######################################################
# SUT Section
######################################################
#include: xci_8.inc
#  ----- Begin inclusion of 'xci_8.inc'
############################################################################
######################################################
# SUT Section
######################################################
# General SUT info
system_vendor      = Cray
system_name        = Isambard 2: XC50 (ThunderX2)
hw_avail           = May-2018
sw_avail           = Mar-2020

# Computation node info
# [Node_Description: Hardware]
node_compute_syslbl = ThunderX2
node_compute_order = N/A
node_compute_count = 8
node_compute_purpose = Compute
node_compute_hw_vendor = N/A
node_compute_hw_model = N/A
node_compute_hw_cpu_name = Marvell ThunderX2 CN9980
node_compute_hw_ncpuorder = N/A
node_compute_hw_nchips = 2
node_compute_hw_ncores = 64
node_compute_hw_ncoresperchip = 32
node_compute_hw_nthreadspercore = 4
node_compute_hw_cpu_char = Permanent turbo to 2.5 GHz
node_compute_hw_cpu_mhz = 2100
node_compute_hw_pcache = 32 KB I + 32 KB D on chip per core
node_compute_hw_scache = 256 KB I+D on chip per core
node_compute_hw_tcache000= 32 MB I+D on chip per chip
node_compute_hw_tcache001 = 0.5 MB shared / 64 cores
node_compute_hw_ocache = None
node_compute_hw_memory = 256 GB (8 x 32 GB)
node_compute_hw_other = None

#[Node_Description: Accelerator]
node_compute_hw_accel_model = N/A
node_compute_hw_accel_count = N/A
node_compute_hw_accel_vendor= N/A
node_compute_hw_accel_type  = N/A
node_compute_hw_accel_connect = N/A
node_compute_hw_accel_ecc    = N/A
node_compute_hw_accel_desc   = N/A

#[Node_Description: Software]
node_compute_hw_adapter_fs_model = None
node_compute_hw_adapter_fs_count = 0
node_compute_hw_adapter_fs_slot_type = None
node_compute_hw_adapter_fs_data_rate = None
node_compute_hw_adapter_fs_ports_used = 0
node_compute_hw_adapter_fs_interconnect = None
node_compute_hw_adapter_fs_driver = None
node_compute_hw_adapter_fs_firmware = None
node_compute_sw_os000 = SUSE Linux Enterprise Server 15 SP1
node_compute_sw_os001 = Linux 4.12.14-197.7_5.0.99-cray_ari_s
node_compute_sw_localfile = xfs
node_compute_sw_sharedfile = None
node_compute_sw_state = Multi-user, run level 3
node_compute_sw_other = None

#[Fileserver]

#[Interconnect]
interconnect_fs_syslbl = Cray Aries
interconnect_fs_order = 0
interconnect_fs_purpose = MPI Traffic
interconnect_fs_hw_vendor = Cray
interconnect_fs_hw_model = N/A
interconnect_fs_hw_switch_fs_model000= N/A
interconnect_fs_hw_switch_fs_model001 = N/A
interconnect_fs_hw_switch_fs_count = N/A
interconnect_fs_hw_switch_fs_ports = N/A
interconnect_fs_hw_topo = Dragonfly
interconnect_fs_hw_switch_fs_data_rate = 14 Gb/s
interconnect_fs_hw_switch_fs_firmware = N/A

#######################################################################
# End of SUT section
# If this config file were to be applied to several SUTs, edits would
# be needed only ABOVE this point.
######################################################################
# ---- End inclusion of '/home/br-spollard/SPEChpc/config/xci_8.inc'

#[Software]
sw_compiler000 = HPE Cray Programming Environment (CPE),
sw_compiler001 = C/C++/Fortran: GCC Version 9.3.0
sw_mpi_library000 = HPE Cray Programming Environment (CPE),
sw_mpi_library001 = Cray-mvapich2 Version 2.3.6
#sw_mpi_other = None
#sw_other = None

node_compute_sw_os000 = SUSE Linux Enterprise Server 15 SP1
node_compute_sw_os001 = Linux 4.12.14-197.7_5.0.99-cray_ari_s

#[Submit notes]
#notes_submit_000 = MPI startup command:
#notes_submit_005 =   aprun command was used to start MPI jobs.

#######################################################################
# End of SUT section
######################################################################

######################################################################
# The header section of the config file.  Must appear
# before any instances of "section markers" (see below)
#
# ext = how the binaries you generated will be identified
# tune = specify "base" or "peak" or "all"
label         = %{label}_%{model}
tune          = base
output_format = text
use_submit_for_speed = 1

# Compiler Settings
default:
CC           = cc
CXX          = CC
FC           = ftn

# Compiler Version Flags
CC_VERSION_OPTION  = --version
CXX_VERSION_OPTION = --version
FC_VERSION_OPTION  = --version

# MPI options and binding environment, dependent upon Model being run
# Adjust to match your system

# MPI (CPU) settings
%if %{model} eq 'mpi'
   APRUN_OPTS = -d 1 -N 64 -q
%endif

# MPI+OpenMP (CPU) settings
%if %{model} eq 'omp'
   APRUN_OPTS = -d $threads -N 1 -q
%endif

#submit = srun -u -n $ranks ${MPIRUN_OPTS} $command
submit = aprun -n $ranks ${APRUN_OPTS} $command

#######################################################################
# Optimization

# Note that SPEC baseline rules require that all uses of a given compiler
# use the same flags in the same order. See the SPEChpc Run Rules
# for more details
#      http://www.spec.org/hpc2021/Docs/runrules.html
#
# OPTIMIZE    = flags applicable to all compilers
# FOPTIMIZE   = flags applicable to the Fortran compiler
# COPTIMIZE   = flags applicable to the C compiler
# CXXOPTIMIZE = flags applicable to the C++ compiler
#
# See your compiler manual for information on the flags available
# for your compiler

# Compiler flags applied to all models
default=base=default:
OPTIMIZE       = -Ofast

# MPI (CPU) settings/flags
%if %{model} eq 'mpi'
   pmodel = MPI
%endif

# MPI+OpenMP (CPU) settings/flags
%if %{model} eq 'omp'
   pmodel = OMP
   OPTIMIZE += -fopenmp
%endif

# No peak flags set, so make peak use the same flags as base
default=peak=default:
basepeak=1

#######################################################################
# Portability
#######################################################################

