SPEC HPC(r) 2021 Benchmark Suites
Copyright 1995-2021 Standard Performance Evaluation Corporation (SPEC)

runhpc v.unknown
Using 'linux-aarch64' tools
Reading file manifests... read 14938 entries from 2 files in 0.27s (55001 files/s)
Loading runhpc modules.................
Locating benchmarks...found 31 benchmarks in 5 benchsets.
Reading config file '/home/br-spollard/SPEChpc/config/hpe_cpe.cfg'
Reading included config file '/home/br-spollard/SPEChpc/config/Example_SUT.inc'
WARNING: Duplicate setting for "node_compute_sw_os000" on line 46
         of /home/br-spollard/SPEChpc/config/hpe_cpe.cfg
WARNING: Duplicate setting for "node_compute_sw_os001" on line 47
         of /home/br-spollard/SPEChpc/config/hpe_cpe.cfg

WARNING: OMP_NUM_THREADS is set to '64' in the run environment.  This
         value will not be used; the tools set a value for this for each
         benchmark run based on the run mode (rate or speed) and the setting
         for number of threads.
         See https://www.spec.org/hpc2021/Docs/config.html#threads
         and https://www.spec.org/hpc2021/Docs/runhpc.html#threads
Retrieving flags file (/home/br-spollard/SPEChpc/config/flags/Example_hpe_cpe_flags.xml)...
  '505.lbm_t' removed
  '519.clvleaf_t' removed
  '521.miniswp_t' removed
  '528.pot3d_t' removed
  '534.hpgmgfv_t' removed
  '535.weather_t' removed


1 configuration selected:

 Action    Run Mode   Workload     Report Type     Benchmarks
--------   --------   --------   ---------------   ----------------------------
validate   speed      test       SPEChpc2021_tny   513.soma_t, 518.tealeaf_t,  
                                                   532.sph_exa_t               
-------------------------------------------------------------------------------

Benchmarks selected: 513.soma_t, 518.tealeaf_t, 532.sph_exa_t
Compiling Binaries
  Up to date 513.soma_t base cce_mpi
  Up to date 518.tealeaf_t base cce_mpi
  Up to date 532.sph_exa_t base cce_mpi


Setting Up Run Directories
  Setting up 513.soma_t test base cce_mpi: run_base_test_cce_mpi.0000
  Setting up 518.tealeaf_t test base cce_mpi: run_base_test_cce_mpi.0000
  Setting up 532.sph_exa_t test base cce_mpi: run_base_test_cce_mpi.0000
Running Benchmarks
  Running 513.soma_t test base cce_mpi [2022-06-20 10:12:23]
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/513.soma_t/run/run_base_test_cce_mpi.0000 -f speccmds.cmd -q -e speccmds.err -o speccmds.stdout
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/513.soma_t/run/run_base_test_cce_mpi.0000 -f compare.cmd -E -e compare.err -o compare.stdout
  Running 518.tealeaf_t test base cce_mpi [2022-06-20 10:12:29]
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/518.tealeaf_t/run/run_base_test_cce_mpi.0000 -f speccmds.cmd -q -e speccmds.err -o speccmds.stdout
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/518.tealeaf_t/run/run_base_test_cce_mpi.0000 -f compare.cmd -E -e compare.err -o compare.stdout
  Running 532.sph_exa_t test base cce_mpi [2022-06-20 10:12:36]
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/532.sph_exa_t/run/run_base_test_cce_mpi.0000 -f speccmds.cmd -q -e speccmds.err -o speccmds.stdout
/home/br-spollard/SPEChpc/bin/specinvoke -d /home/br-spollard/SPEChpc/benchspec/HPC/532.sph_exa_t/run/run_base_test_cce_mpi.0000 -f compare.cmd -E -e compare.err -o compare.stdout
Success: 1x513.soma_t 1x518.tealeaf_t 1x532.sph_exa_t
Producing Raw Reports
 label: cce_mpi
  workload: test
   metric: SPEChpc2021_tny_base
    format: raw -> /home/br-spollard/SPEChpc/result/hpc2021_tny.035.tiny.test.rsf
Parsing flags for 513.soma_t base: done
Parsing flags for 518.tealeaf_t base: done
Parsing flags for 532.sph_exa_t base: done
Doing flag reduction: done
    format: Text -> /home/br-spollard/SPEChpc/result/hpc2021_tny.035.tiny.test.txt

The log for this run is in /home/br-spollard/SPEChpc/result/hpc2021.035.log

runhpc finished at 2022-06-20 10:12:49; 34 total seconds elapsed
============================= PBS epilogue =============================

End of Job Report
Run at 2022-06-20 10:12:55 for job 212285.xci00
Submitted              : 2022-06-20 10:12:13
Queued                 : 2022-06-20 10:12:13
Started                : 2022-06-20 10:12:13
Completed              : 2022-06-20 10:12:55
Queued Time            : 0:00:00 (0 seconds)
Elapsed Time           : 0:00:37 (37 seconds, 6% of limit)
Walltime Limit         : 0:10:00 (600 seconds)
Node Time Limit        : 0:10:00 (600 seconds)
Node Time              : 0:00:37 (37 seconds, 6% of limit)
Job Name               : jobsubmit.pbs
Queue                  : arm
Owner                  : br-spollard
Group                  : -default-
Project                : GW03
Subproject             : 
Funding                : 
Trustzone              : 
STDOUT                 : /home/br-spollard/benchmarks/testing/jobsubmit.pbs.o212285
STDERR                 : /home/br-spollard/benchmarks/testing/jobsubmit.pbs.e212285
Job Directory          : /home/br-spollard/pbs.212285.xci00.x8z
Job Arch               : XT
CPU Core Type          : arm
Total Nodes            : 1
Total Tasks            : 64
Parent Node            : xcimom
Parent Node Memory     : 
Parent Node CPU Time   : 
Compute Nodes          : 313
Electrical Groups      : 1
Run Version            : 1

For more information see https://gw4-isambard.github.io/docs/
